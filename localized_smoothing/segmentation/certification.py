"""This module contains functionality for certifying model robustness.

### Main methods ###

certify_dataset: Applies certificates to multiple images
    from a dataset for multiple adversarial budgets.
certify_image: Applies certificates to a single image with multiple adversarial budgets.

### Baselines / base certificates ###

calc_max_rads_argmax: Calculates maximum certified radii sqrt(eta) of
    majority voting (i.e. Cohen et al., 2019).
calc_n_perturbed_center: Calculates certified l_0 output distance of center smoothing
    for specified budgets.
calc_abstain_center: Determines whether center smoothed model should abstain from making prediction.

### Evaluation of baselines / base certificates ###

calc_naive_certified_confusions: Calculates per-budget certified confusion matrices
    with abstention and ignore as extra class.
calc_abstained_confusion: Calculates confusion matrix
    with abstention and ignore label as extra classes.


### Collective certification procedure ###

generate_collective_lp: Constructs the collective LP from Appendix E.4.
evaluate_collective_lp: Evaluates collective linear program for specific budgets and
    base certified radii.
quantize_max_rads: Quantizes maximum certifiable radii and generates corresponding prediction mask.
    This method is used for generating parameter values for the collective linear program
    generated by generate_collective_lp().

### Misc ###
calc_max_grid_size: Computes maximum number of pixels within a single grid cell
    for localized smoothing.
"""

import cvxpy as cp
import numpy as np
import torch
from cvxpy import SolverError
from numpy.typing import NDArray
from scipy.stats import norm
from statsmodels.stats.proportion import proportion_confint
from torch.utils.data import Dataset
from tqdm import tqdm

from localized_smoothing.segmentation.smoothing import SmoothedModel
from localized_smoothing.segmentation.utils import (confusion_matrix,
                                                    generate_distance_mask,
                                                    generate_grid,
                                                    generate_smoothing_stds,
                                                    get_center_crop_coords)


def certify_dataset(dataset: Dataset,
                    img_idx: list[int],
                    original_shape: NDArray[np.int_],
                    model: torch.nn.Module,
                    normalization_params: dict,
                    sample_params: dict,
                    distribution_params: dict,
                    certification_params: dict,
                    device: torch.device,
                    cert_file: str = None):
    """Applies certificates to multiple images from a dataset for multiple adversarial budgets.

    This method assumes, that all images have the same resolution
    (otherwise, we would not be able to reuse the same cvxpy optimization problem,
    which we need for efficiency).

    Args:
        dataset: The dataset.
        img_idx: List of dataset indices that the certificates should be applied to.
        original_shape: Resolution [H_orig, W_orig], which should match original shape
            of all images in img_idx.
        model: Segmentation model to certify.
        normalization_params: Dictionary with two keys "mean" and "std", each corresponding
            to Tensor of shape [3] indicating channelwise mean and std for z-normalization.
        sample_params: sample_params: Dictionary containing number of samples etc.,
            see "seml/scripts/segmentation/cert.py".
        distribution_params: Dictionary containing parameters of the smoothing distribution
            see "seml/scripts/segmentation/cert.py".
        certification_params: Parameters for the different certification procedures,
            see "seml/scripts/segmentation/cert.py".
        device: The device the model is on.
        cert_file: File path the certificates should be stored in after each image.

    Returns:
        A nested dictionary for N images with N+1 keys:
        - The first key is "budget" and contains an array of B adversarial budgets
        - The other keys are integer image ids
            - Each of these contains one dictionary per certificate (e.g. "argmax_holm")
                in certification_params['naive_certs'] and certification_params['base_certs']
                Each entry has the following keys:
                - For certificates starting with 'argmax':
                    - "raw_confusion": A (C+1)x(C+1) confusion matrix
                        for the smoothed prediction on this image,
                        ignoring abstention by the smoothed classifier.
                        "Ignore" class C is an additional class that is never predicted.
                    - "abstained_confusion": A (C+2)x(C+2)confusion matrix
                        for the smoothed prediction on this image,
                        with abstention of the smoothed classifier as an additional class C.
                        "Ignore" class C+1 is an additional class that is never predicted.
                    - "naive": An array of shape Bx(C*2+2)x(C*2+2).
                        The first dimension corresponds to the budgets in "budget" (see above).
                        The trailing dimensions are a confusion matrix where the classes are:
                            C certified classes, C uncertified classes, Abstain, Ignore label.
                        The ground truth targets are always certified classes {0,...,C-1}.
                        The uncertified classes and the abstain class are never in the ground truth.
                        The predicions are either certified classes {0,...,C-1}, uncertified classes
                        {C,...,2*C-1} or abstain class 2*C.
                        The "ignore" class 2*C+1 is a class that is never predicted.
                    - "collective" (only for certificates from certification_params['base_certs']):
                        A dict with four entries:
                        - "all": Array of length B,
                            with each entry corresponding to number of collectively
                            certified predictions when applying collective certificate to all
                            predictions that are not already certified by base certificate.
                        - "correct_only": Array of length B,
                            with each entry corresponding to number of collectively
                            certified predictions when applying collective certificate to all
                            correct predictions that are not already certified by base certificate.
                        - "solver_fails_all": Boollean array of length B, indicating if
                            applying cvxpy solver to all predictions failed.
                            If True, then entry in "all" should be 0.
                        - "solver_fails_correct_only": Boolean array of length B, indicating if
                            applying cvxpy_solver to all correct predictions failed.
                            If True, then entry in "correct_only" should be 0.
                - For certificates starting with "center"
                    - "abstain": Whether the smoothed model abstains (True) or not (False)
                        from the entire segmentation mask.
                    - "n_perturbed": 1D array of certified l_0 output distances,
                        with one entry per adversarial budget.
    """

    model.eval()

    budgets = np.linspace(certification_params['budget_min'],
                          certification_params['budget_max'],
                          certification_params['budget_steps'])

    for i in img_idx:
        if not np.all(dataset[i][2].numpy() == original_shape):
            raise ValueError(
                f'All images in img_idx must have original shape {original_shape}'
            )

    cert_dict = {'budgets': budgets}

    if certification_params['n_max_rad_bins'] is None:
        n_max_rad_bins = calc_max_grid_size(original_shape, distribution_params)
    else:
        n_max_rad_bins = certification_params['n_max_rad_bins']

    # Create problem out here, so can be pre-compiled once for all images -> Much faster
    (collective_problem, squared_budget_param, squared_max_rads_quantized_param,
     prediction_mask_quantized_param) = generate_collective_lp(
         distribution_params, n_max_rad_bins)

    collective_problem_params = {
        'squared_budget': squared_budget_param,
        'squared_max_rads_quantized': squared_max_rads_quantized_param,
        'prediction_mask_quantized': prediction_mask_quantized_param
    }

    smoothed_model = SmoothedModel(model, sample_params, distribution_params,
                                   normalization_params, device)

    for i in tqdm(img_idx[:certification_params['n_images']]):
        img, targets, _ = dataset[i]
        assert img.max() <= 1.0

        x1, y1, x2, y2 = get_center_crop_coords(int(targets.shape[0]),
                                                int(targets.shape[1]),
                                                int(original_shape[0]),
                                                int(original_shape[1]))

        targets_unpadded = targets[y1:y2, x1:x2].numpy()

        image_cert_dict = certify_image(img, targets_unpadded, original_shape,
                                        smoothed_model, budgets,
                                        certification_params, n_max_rad_bins,
                                        collective_problem,
                                        collective_problem_params, device)

        cert_dict[i] = image_cert_dict

        if cert_file is not None:
            torch.save(cert_dict, cert_file)

    return cert_dict


def certify_image(img: torch.Tensor, targets_unpadded: torch.Tensor,
                  original_shape: NDArray[np.int_],
                  smoothed_model: SmoothedModel, budgets: NDArray[np.float_],
                  certification_params: dict, n_max_rad_bins: int,
                  collective_problem: cp.Problem,
                  collective_problem_params: dict[str, cp.Parameter],
                  device: torch.device):
    """Applies certificates to a single image with multiple adversarial budgets.

    Args:
        img: A single padded image of shape (H_orig + H_pad) x (W_orig + W_pad).
        targets_unpadded: A target segmentation mask of shape H_orig x W_orig.
        original_shape: Shape [H_orig, W_orig] of the image, without padding.
        model: Segmentation model to certify.
        budgets: 1D array of adversarial l_2 budgets.
        sample_params: sample_params: Dictionary containing number of samples etc.,
            see "seml/scripts/segmentation/cert.py".
        distribution_params: Dictionary containing parameters of the smoothing distribution
            see "seml/scripts/segmentation/cert.py".
        certification_params: Parameters for the different certification procedures,
            see "seml/scripts/segmentation/cert.py".
        n_max_rad_bins: Number of quantization bins for base certificate radii,
            see Appendix E.3.
        collective_problem: Collective linear program, as constructed via
            generate_collective_lp.
        collective_problem_params: Dictionary of parameters for this linear program,
            as returned by generate_collective_lp.
        device: The device the model is on.

    Returns:
        Nested dictionary with one key per certificate (e.g. "argmax_holm")
            in certification_params['naive_certs'] and certification_params['base_certs'].
            Each entry is a dictionary with the following keys:
            - For certificates starting with 'argmax':
                - "raw_confusion": A (C+1)x(C+1) confusion matrix
                    for the smoothed prediction on this image,
                    ignoring abstention by the smoothed classifier.
                    "Ignore" class C is an additional class that is never predicted.
                - "abstained_confusion": A (C+2)x(C+2)confusion matrix
                    for the smoothed prediction on this image,
                    with abstention of the smoothed classifier as an additional class C.
                    "Ignore" class C+1 is an additional class that is never predicted.
                - "naive": An array of shape Bx(C*2+2)x(C*2+2).
                    The first dimension corresponds to the budgets in "budget" (see above).
                    The trailing dimensions are a confusion matrix where the classes are:
                        C certified classes, C uncertified classes, Abstain, Ignore label.
                    The ground truth targets are always certified classes {0,...,C-1}.
                    The uncertified classes and the abstain class are never in the ground truth.
                    The predicions are either certified classes {0,...,C-1}, uncertified classes
                    {C,...,2*C-1} or abstain class 2*C.
                    The "ignore" class 2*C+1 is a class that is never predicted.
                - "collective" (only for certificates from certification_params['base_certs']):
                    A dict with four entries:
                    - "all": Array of length B,
                        with each entry corresponding to number of collectively
                        certified predictions when applying collective certificate to all
                        predictions that are not already certified by base certificate.
                    - "correct_only": Array of length B,
                        with each entry corresponding to number of collectively
                        certified predictions when applying collective certificate to all
                        correct predictions that are not already certified by base certificate.
                    - "solver_fails_all": Boollean array of length B, indicating if
                        applying cvxpy solver to all predictions failed.
                        If True, then entry in "all" should be 0.
                    - "solver_fails_correct_only": Boolean array of length B, indicating if
                        applying cvxpy_solver to all correct predictions failed.
                        If True, then entry in "correct_only" should be 0.
            - For certificates starting with "center"
                - "abstain": Whether the smoothed model abstains (True) or not (False)
                    from the entire segmentation mask.
                - "n_perturbed": 1D array of certified l_0 output distances,
                    with one entry per adversarial budget.
    """

    certs = set.union(set(map(str.lower, certification_params['naive_certs'])),
                      set(map(str.lower, certification_params['base_certs'])))

    predict_argmax = any([cert.startswith('argmax') for cert in certs])
    predict_center = any([cert.startswith('center') for cert in certs])

    n_samples_cert = smoothed_model.sample_params['n_samples_cert']

    # Get predictions and sample statistics
    with torch.no_grad():
        pred_dict = smoothed_model.smooth_prediction(img,
                                                     original_shape,
                                                     device,
                                                     argmax=predict_argmax,
                                                     center=predict_center)

        pred_argmax = pred_dict.get('argmax')
        pred_center = pred_dict.get('center')
        pred_center_radius = pred_dict.get('center_radius')

        sample_dict = smoothed_model.sample_base_classifier_statistics(
            img,
            original_shape,
            device,
            sample_consistency=predict_argmax,
            sample_distances=predict_center,
            predictions_argmax=pred_argmax,
            predictions_center=pred_center)

        consistency = sample_dict.get('consistency')
        distances = sample_dict.get('distances')

        if predict_argmax:
            pred_argmax = pred_argmax.cpu().numpy()
            consistency = consistency.detach().cpu().numpy()
            assert np.all(pred_argmax.shape == targets_unpadded.shape)
        if predict_center:
            pred_center = pred_center.cpu().numpy()
            distances = distances.detach().cpu().numpy()
            assert np.all(pred_center.shape == targets_unpadded.shape)

    distribution_params = smoothed_model.distribution_params
    std_min = min(distribution_params['std_min'],
                  distribution_params['std_max'])
    cert_dict = {}

    # Evaluate certificates
    for cert in certs:
        cert_dict[cert] = {}
        alpha = certification_params['alpha']

        if cert.startswith('argmax'):
            assert cert in ['argmax_bonferroni', 'argmax_holm']
            pred = pred_argmax
            holm_correction = ('holm' in cert)
        elif cert.startswith('center'):
            assert cert in ['center_independent', 'center_bonferroni']
            pred = pred_center
            if cert == 'center_bonferroni':
                alpha /= 2
        else:
            raise ValueError('Cert not supported')

        n_classes = smoothed_model.base_model.classes

        cert_dict[cert]['raw_confusion'] = confusion_matrix(
            targets_unpadded, pred, labels=np.arange(n_classes))

        if cert.startswith('center'):
            cert_dict[cert]['abstain'] = calc_abstain_center(
                distances,
                n_samples_cert,
                pred_center_radius,
                alpha,
                delta=certification_params['delta'])

            cert_dict[cert]['n_perturbed'] = calc_n_perturbed_center(
                distances,
                n_samples_cert,
                original_shape,
                budgets,
                std_min,
                alpha,
                delta=certification_params['delta'])

        else:
            max_rads = calc_max_rads_argmax(consistency, n_samples_cert, alpha,
                                            holm_correction)
            abstain_mask = (max_rads <= 0)

            cert_dict[cert]['abstained_confusion'] = calc_abstained_confusion(
                targets_unpadded, pred, abstain_mask, n_classes=n_classes)

            cert_dict[cert]['naive'] = calc_naive_certified_confusions(
                targets_unpadded,
                pred,
                max_rads,
                abstain_mask,
                budgets,
                std_min,
                n_classes=n_classes)

            if cert in map(str.lower, certification_params['base_certs']):

                cert_dict[cert]['collective'] = evaluate_collective_lp(
                    pred,
                    targets_unpadded,
                    max_rads,
                    abstain_mask,
                    budgets,
                    std_min,
                    collective_problem,
                    collective_problem_params,
                    original_shape,
                    distribution_params['grid_shape'],
                    n_max_rad_bins=n_max_rad_bins)

    return cert_dict


def calc_max_rads_argmax(consistency: NDArray[np.int_],
                         n_samples: int,
                         alpha: float,
                         holm_correction: bool = False):
    """Calculates maximum certified radii of majority voting (i.e. Cohen et al., 2019).

    Multiple comparisons error can either be corrected using Bonferroni correction
    (i.e. alpha/n)
    or Holm Correction (i.e. alpha/1, alpha/2, alpha/3, ...).
    See Discussion in Appendix G.4 of our paper.

    Args:
        consistency: Array of shape H_orig x W_orig, indiciating how often
            sampled predictions were identical to smoothed prediction.
        n_samples: Number of Monte Carlo samples taken.
        alpha: Significance, i.e. 1-alpha is probability of certificate holding.
        holm_correction: Whether to use Holm (True) or Bonferroni (False) correction.

    Returns:
        Array of shape H_orig x W_orig with per-pixel certified radii sqrt(eta),
        with eta as in Eq. 2.
    """
    n_pixels = np.prod(consistency.shape)

    if holm_correction:
        order = np.argsort(consistency, axis=None)
        ps_lower = np.ones_like(consistency, dtype='float') * -1

        for i, (row, col) in enumerate(
                zip(*np.unravel_index(order, ps_lower.shape))):
            ps_lower[row, col] = proportion_confint(consistency[row, col],
                                                    n_samples,
                                                    alpha * 2 / (i + 1),
                                                    method='beta')[0]

        assert -1 not in ps_lower

    else:
        ps_lower = proportion_confint(consistency,
                                      n_samples,
                                      alpha * 2 / n_pixels,
                                      method='beta')[0]

    max_rads = norm.ppf(ps_lower)
    max_rads[max_rads < 0] = 0

    return max_rads


def calc_n_perturbed_center(distances: NDArray[np.float_],
                            n_samples: int,
                            original_shape: NDArray[np.int_],
                            budgets: NDArray[np.float_],
                            std_min: float,
                            alpha: float,
                            delta: float = 0.05,
                            beta: float = 2):
    """Calculates certified l_0 output distance of center smoothing for specified budgets.

    See Algorithm 2 of https://arxiv.org/pdf/2102.09701.pdf .

    Our implementation is slightly different from theirs (but equivalent):
        They determine quantile q of the empirical cdf such that
            a w.h.p. lower bound on the true CDF value
            is greater equal 1/2+Delta for q.
        We first compute a w.h.p. lower bound on the true CDF
            and then select its 1/2+Delta quantile.

    Args:
        distances: Array of l_0 distances from sampled segmentation mask to center smoothed
            predicted.
        n_samples: Number of Monte Carlo samples that were taken.
        original_shape: Image shape [H_orig, W_orig] before padding.
        budgets: 1D array of adversarial l_2 budgets.
        std_min: Smallest Gaussian smoothing standard deviation used for randomized smoothing.
        alpha: Significance, i.e. 1-alpha is probability of certificate holding.
        delta: Hyperparameter for probabilistic approximation of 1/2-minimum enclosing ball.
        beta: Approximation factor for 1/2-minimum enclosing ball (see center smoothing paper).

    Returns:
        1D array of same length as "budgets" argument, specifying certified l_0 output distance
            for each of the adversarial budgets.
    """

    assert n_samples == len(distances)

    distances_sorted = np.sort(distances)

    ps = np.linspace(1 / n_samples, 1, n_samples)
    eps = np.sqrt(np.log(2 / alpha) / (2 * n_samples))
    ps_lower = np.maximum(0, ps - eps)

    n_pixels = np.prod(original_shape)
    n_perturbed = []
    for budget in budgets:
        if budget == 0:
            n_perturbed.append(0)
        else:
            # Probabilistic lower bound on output distance CDF, based on empirical CDF.
            ps_worst_case = norm.cdf(norm.ppf(ps_lower) - (budget / std_min))
            if np.all(ps_worst_case < 0.5 + delta):
                n_perturbed.append(n_pixels)
            else:
                # Find 1/2+Delta quantile of lower bound on output distance CDF.
                certified_quantile = (ps_worst_case < 0.5 + delta).argmin()
                n_perturbed.append(
                    min(n_pixels,
                        (1 + beta) * distances_sorted[certified_quantile]))

    return np.array(n_perturbed)


def calc_abstain_center(distances: NDArray[np.float_],
                        n_samples: int,
                        radius: float,
                        alpha: float,
                        delta: float = 0.05):
    """Determines whether center smoothed model should abstain from making prediction.

    See Algorithm 1 in https://arxiv.org/pdf/2102.09701.pdf .
    Note that center smoothing always abstains from the entire segmentation mask, not just
    individual pixels.

    Args:
        distances: Array of l_0 distances from sampled segmentation mask to center smoothed
            predicted.
        n_samples: Number of Monte Carlo samples that were taken.
        radius: Radius of the minimum enclosing ball used for making the smoothed prediction.
        alpha: Significance, i.e. 1-alpha is probability of certificate holding.
        delta: Hyperparameter for probabilistic approximation of 1/2-minimum enclosing ball.

    Returns:
        Whether to abstain (True) or not (False)
    """

    # Not 2/alpha, because do Bonferroni correction externally
    delta_1 = np.sqrt(np.log(1 / alpha) / (2 * n_samples))
    p_delta_1 = np.sum(distances <= radius) / n_samples - delta_1
    delta_2 = (1 / 2) - p_delta_1

    if max(delta_1, delta_2) <= delta:
        return False  # Don't abstain
    else:
        return True  # Abstain


def calc_naive_certified_confusions(targets: NDArray[np.int_],
                                    pred: NDArray[np.int_],
                                    max_rads: NDArray[np.float_],
                                    abstain_mask: NDArray[np.bool_],
                                    budgets: NDArray[np.float_],
                                    std_min: float,
                                    n_classes: int = 21) -> NDArray[np.int_]:
    """Calculates per-budget certified confusion matrices with abstention and ignore as extra class.

    This method assumes naive collective certificates
    that employ Gaussian smoothing (i.e. w=sigma^2 in Eq. 2)

    Args:
        targets: Integer ground truth labels of arbitrary shape
        pred: Predicted labels of same shape as targets
        max_rads: Maximum certifiable radii sqrt(eta) (with eta as in Eq. 2)
            of same shape as targets.
        abstain mask: Boolean array indicating for which predictions the smoothed model abstains
            (True means abstain, False means do not abstain).
        budgets: 1D array of adversarial l_2 budgets.
        std_min: Smallest Gaussian smoothing standard deviation used for randomized smoothing.
        n_classes: Number of class labels.

    Returns:
        array of shape Bx(C*2+2)x(C*2+2).
            The first dimension corresponds to the budgets in "budgets" (see above).
            The trailing dimensions are a confusion matrix where the classes are:
                C certified classes, C uncertified classes, Abstain, Ignore label.
            The ground truth targets are always certified classes {0,...,C-1}.
            The uncertified classes and the abstain class are never in the ground truth.
            The predicions are either certified classes {0,...,C-1}, uncertified classes
            {C,...,2*C-1} or abstain class 2*C.
            The "ignore" class 2*C+1 is a class that is never predicted.
    """

    assert np.all(pred.shape == targets.shape)
    confs = []

    for budget in budgets:
        pred_modified = pred.copy()
        if budget == 0:
            robust = ~abstain_mask
        else:
            robust = budget / std_min < max_rads  # std_min = perturbation in lowest noise region
        pred_modified[abstain_mask] = 2 * n_classes  # n_classes*2 = Abstain
        # n_classes to 2*n_classes - 1: Not abstaining, but not robust either
        pred_modified[~abstain_mask & ~robust] += n_classes

        confs.append(
            confusion_matrix(targets,
                             pred_modified,
                             labels=np.arange(2 * n_classes + 1)))

    return np.stack(confs, axis=0)


def calc_abstained_confusion(targets: NDArray[np.int_],
                             pred: NDArray[np.int_],
                             abstain_mask: NDArray[np.bool_],
                             n_classes: int = 21) -> NDArray[np.int_]:
    """Calculates confusion matrix with abstention and ignore label as extra classes.

    Args:
        targets: Integer ground truth labels of arbitrary shape
        pred: Predicted labels of same shape as targets
        abstain: Boolean array indicating for which predictions the smoothed model abstains
            (True means abstain, False means do not abstain).
        n_classes: Number of class labels.

    Returns:
        Confusion matrix of shape (n_classes+2) ** 2,
        where entry (i, j) is number of predictions that have ground truth class i
        and are classified as j.
        Class n_classes corresponds to abstention and never is a ground truth.
        Class n_classes+1 corresponds to the ignore label (255) that appears in some target masks
        and is never predicted.
    """

    assert np.all(pred.shape == targets.shape)
    pred_modified = pred.copy()
    pred_modified[abstain_mask] = n_classes

    return confusion_matrix(targets,
                            pred_modified,
                            labels=np.arange(n_classes + 1))


def generate_collective_lp(distribution_params: dict, n_max_rad_bins: int):
    """Constructs the collective LP from Appendix E.4.

    As discussed in Appendix E, we quantize the squared certified radii eta
    and exploit that the smoothing standard deviations are constant within
    each grid cell, so that we only need to optimize over variables of shape
    (n_grid_rows * n_grid_cols) x n_max_rad_bins.
    See also quantize_max_rads() below.

    cvxpy does not work with strict inequalities, so we maximize
    the number of predictions for which w*b >= eta instead of
    minimizing the number of predictions for which w*b < eta.

    Args:
        distribution_params: Dictionary containing parameters of the smoothing distribution
            see "seml/scripts/segmentation/cert.py".
        n_max_rad_bins: Number of quantization bins for base certificate radii,
            see Appendix E.3.
        milp: Whether to solve exactly.

    Returns:
        A tuple with four elements
            1.) The linear program.
            2.) Scalar parameter for specifying squared budget epsilon^2 from Eq. 5 in Theorem 4.2.
            3.) Parameter for specifying quantization thresholds for squared certified radii eta,
                i.e. this parameter coresponds to threshold matrix E from Appendix E.2/3.
                Shape (n_grid_rows * n_grid_cols) x n_max_rad_bins.
            4.) Parameter of shape (n_grid_rows * n_grid_cols) x n_max_rad_bins,
                with entry (i * n_grid_cols + j, b) specifying how many preditions from set T
                have a squared certifiable radius in quantization bin b.
                Corresponds to the set cardinality in Eq. 23.
    """

    # [cells_H, cells_W]
    grid_shape = np.array(distribution_params['grid_shape'])

    # scalar
    squared_budget = cp.Parameter()
    # (n_grid_rows * n_grid_cols) x n_max_rad_bins matrices
    squared_max_rads_quantized = cp.Parameter(
        (np.prod(grid_shape), n_max_rad_bins))
    prediction_mask_quantized = cp.Parameter(
        (np.prod(grid_shape), n_max_rad_bins))

    weights_flattened = []
    for i, j in np.ndindex(*grid_shape):

        #  (..., grid_shape, *generate_grid(grid_shape, grid_shape), ...)
        #  gives us per-grid-cell stds instead of per-pixel stds,
        #  i.e. we exploit the sharing of noise levels as in Appendix E. 3.
        stds = generate_smoothing_stds(
            i, j, grid_shape, *generate_grid(grid_shape, grid_shape),
            distribution_params['std_min'], distribution_params['std_max'],
            distribution_params['metric'],
            distribution_params['interpolate_variance'],
            distribution_params['max_std_at_boundary'])

        grid_weights = (1 / stds)**2

        if distribution_params['mask_distance'] is not None:
            distance_mask = generate_distance_mask(
                i, j, distribution_params['mask_distance'], grid_shape,
                *generate_grid(grid_shape, grid_shape),
                distribution_params['metric'])

            grid_weights[~distance_mask] = 0

        weights_flattened.append(grid_weights.flatten())

    # (n_grid_rows * n_grid_cols) x n_max_rad_bins matrix
    weights_flattened = np.vstack(weights_flattened)

    # Budget allocation variable, one per grid cell.
    bs = cp.Variable(grid_shape[0] * grid_shape[1])

    # (n_grid_rows * n_grid_cols) vector
    bs_perceived = weights_flattened @ bs

    attacked = cp.Variable((np.prod(grid_shape), n_max_rad_bins), boolean=False)
    constraints = [
        attacked >= 0, attacked <= 1, bs >= 0,
        cp.sum(bs) <= squared_budget
    ]

    # This is Eq. 5.
    x = cp.multiply(squared_max_rads_quantized, attacked)
    constraints.append(x <= bs_perceived[:, None])

    objective = cp.sum(cp.multiply(attacked, prediction_mask_quantized))
    problem = cp.Problem(cp.Maximize(objective), constraints)

    return problem, squared_budget, squared_max_rads_quantized, prediction_mask_quantized


def evaluate_collective_lp(pred: NDArray[np.int_],
                           targets: NDArray[np.int_],
                           max_rads: NDArray[np.float_],
                           abstain_mask: NDArray[np.bool_],
                           budgets: NDArray[np.float_],
                           std_min: float,
                           problem: cp.Problem,
                           problem_params: dict[str, cp.Parameter],
                           original_shape: NDArray[np.int_],
                           grid_shape: NDArray[np.int_],
                           n_max_rad_bins: int = None,
                           certify_all: bool = True,
                           certify_correct: bool = True):
    """Evaluates collective linear program for specific budgets and base certified radii.

    The collective LP can be evaluated for different sets of predictions T
    (see Theorem 4.2 and Eq. 28 in Appendix E.4.)

    We consider one or both of the following sets T:
        - Predictions that do not abstain and are not already certified by the base certificates
            (certify_all), see Eq. 28 in Appendix E.4.
        - Predictions that are correct, do not abstain and are  already certified by
            the base certificates (certify_correct), see Eq. 28 in Appendix E.4.

    To obtain the overall number of (correct) robust predictions, you must add the values from
    the naive certificates!

    Args:
        pred: Smoothed prediction of shape H_orig x W_orig.
        targets: Ground truth segmentation mask of shape H_orig x W_orig.
        max_rads: Array of maximum certified radii sqrt(eta) (with eta as in Eq. 2).
            of shape H_orig x W_orig.
        abstain_mask: Boolean array indicating from which per-pixel prediction
            the smoothed model abstains (True), shape H_orig x W_orig.
        budgets: 1D array of adversarial l_2 budgets.
        std_min: Smallest smoothing standard deviation used by the localized smoothing
            distributions.
        problem: Collective linear program, as constructed via
            generate_collective_lp.
        problem_params: Dictionary of parameters for this linear program,
            as returned by generate_collective_lp.
        original_shape: Shape [H_orig, W_orig].
        grid_shape: 1D array of length 2, specifying the vertical and horizontal number
            of grid cells, respectively.
        n_max_rad_bins: Number of quantization bins for base certificate radii,
            see Appendix E.3.
        certify_all: True. Whether to apply the LP to all predictions
            that do not abstain and are not already certified by the base certificates.
        certify_correct: True. Whether to apply the LP to all predictions
            that are correct and not already certified by the base certificates.

    Returns:
        Dictionary with up to four keys:
            - "all": Array of length len(budgets) with number of predictions that are robust,
                on top of those that are not already certified by base certificates.
            - "correct_only": Array of length len(budgets) with number of correct predictions
                that are robust,
                on top of those that are not already certified by base certificates.
            - "solver_fails_all": Boollean array of length B, indicating if
                applying cvxpy solver to all predictions failed.
                If True, then entry in "all" should be 0.
            - "solver_fails_correct_only": Boolean array of length B, indicating if
                applying cvxpy_solver to all correct predictions failed.
                If True, then entry in "correct_only" should be 0.
    """

    assert certify_all or certify_correct
    cert_dict = {}
    cert_dict['all'] = []
    cert_dict['correct_only'] = []
    cert_dict['solver_fails_all'] = []
    cert_dict['solver_fails_correct_only'] = []

    for budget in budgets:
        naively_certified_mask = budget / std_min < max_rads
        problem_params['squared_budget'].value = budget**2

        for cert_type in ['all', 'correct_only']:

            if cert_type == 'all':
                if not certify_all:
                    continue
                prediction_mask = (~naively_certified_mask & ~abstain_mask)

            elif cert_type == 'correct_only':
                if not certify_correct:
                    continue
                prediction_mask = (~naively_certified_mask & ~abstain_mask &
                                   (pred == targets))

            else:
                assert False

            max_rads_quantized, prediction_mask_quantized = quantize_max_rads(
                max_rads,
                prediction_mask,
                original_shape,
                grid_shape,
                n_bins=n_max_rad_bins)

            problem_params[
                'squared_max_rads_quantized'].value = max_rads_quantized**2
            problem_params[
                'prediction_mask_quantized'].value = prediction_mask_quantized

            if budget == 0:
                n_certified = 0  # 0, because only consider additional benefit from collective
            else:
                try:
                    n_certified = prediction_mask.sum() - problem.solve(
                        solver='MOSEK')
                    cert_dict[f'solver_fails_{cert_type}'].append(False)
                except SolverError:
                    n_certified = 0
                    cert_dict[f'solver_fails_{cert_type}'].append(True)

            cert_dict[cert_type].append(n_certified)

    cert_dict = {
        k: np.array(cert_dict[k]) for k in cert_dict if len(cert_dict[k]) > 0
    }

    return cert_dict


def quantize_max_rads(max_rads: NDArray[np.float_],
                      prediction_mask: NDArray[np.bool_],
                      original_shape: NDArray[np.int_],
                      grid_shape: NDArray[np.int_],
                      n_bins: int = 3):
    """Quantizes maximum certifiable radii and generates corresponding prediction mask.

    This method is used for generating parameter values for the collective linear program
    generated by generate_collective_lp().

    Args:
        max_rads: Array of shape H_orig x W_orig specifying the maximum certifiable
            radii sqrt(eta), with eta as in Eq. 2.
        prediction_mask: Boolean mask of H_orig x W_orig indicating for which
            predictions the collective certificate is to be evaluated.
            Corresponds to set T in Theorem 4.2.
        original_shape: 1D array specifying the original shape [H_orig, W_orig]
        grid_shape: 1D array specifying [n_grid_rows, n_grid_cols] for the partitioning
            of the image into grid cells (see Fig. 1.).
        n_bins: The number of quantization bins.

    Returns:
        Tuple consisting of two arrays
            1.) Array of shape (n_grid_rows * n_grid_cols) x n_bins,
                with entry (i * n_grid_cols + j, b) specifying the bth quantization threshold
                for the radii. Corresponds to matrix E in Appendix E.2/E.3.
            2.) Array of shape (n_grid_rows * n_grid_cols) x n_bins,
                with entry (i * n_grid_cols + j, b) specifying the number of pixels (k, l)
                in grid cell (i, j) for which prediction_mask[k, l] == True
                and max_rads[k, l] is in the bth quantization bin.
                Corresponds to the set cardinality in Eq. 23.
    """

    max_rads_quantized = []
    prediction_mask_quantized = []
    grid_vertical, grid_horizontal = generate_grid(original_shape, grid_shape)

    assert n_bins is not None

    for i, j in np.ndindex(*grid_shape):
        prediction_mask_grid = prediction_mask[
            grid_vertical[i][0]:grid_vertical[i][1],
            grid_horizontal[j][0]:grid_horizontal[j][1]]

        max_rads_grid = max_rads[grid_vertical[i][0]:grid_vertical[i][1],
                                 grid_horizontal[j][0]:grid_horizontal[j][1]]

        if prediction_mask_grid.sum() <= n_bins:
            max_rads_grid_quantized = np.zeros(n_bins)
            max_rads_grid_quantized[:prediction_mask_grid.sum(
            )] = max_rads_grid[prediction_mask_grid]

            prediction_mask_grid_quantized = np.zeros(n_bins)
            prediction_mask_grid_quantized[:prediction_mask_grid.sum()] = 1

        else:
            # Generate equally spaced thresholds between min and max max_rad
            thresholds = np.linspace(max_rads_grid[prediction_mask_grid].min(),
                                     max_rads_grid[prediction_mask_grid].max(),
                                     n_bins)

            thresholds = np.append(thresholds, thresholds.max() + 1)

            assert thresholds.min() <= max_rads_grid[prediction_mask_grid].min()

            # Generate new max_rads array corresponding to the quantization bins / thresholds
            max_rads_grid_quantized = thresholds[:-1]

            # Count how many max_rads fall into each bin, store data in new prediction mask
            prediction_mask_grid_quantized = np.histogram(
                max_rads_grid[prediction_mask_grid], thresholds)[0]

        assert prediction_mask_grid_quantized.sum() == prediction_mask_grid.sum(
        )

        max_rads_quantized.append(max_rads_grid_quantized)

        prediction_mask_quantized.append(prediction_mask_grid_quantized)

    return np.vstack(max_rads_quantized), np.vstack(prediction_mask_quantized)


def calc_max_grid_size(original_shape: NDArray[np.int_],
                       distribution_params: dict) -> int:
    """Computes maximum number of pixels within a single grid cell for localized smoothing.

    Args:
        original_shape: Original shape [H_orig, W_orig] of an input image, before padding.
        distribution_params: Dictionary containing parameters of the smoothing distribution
            see "seml/scripts/segmentation/cert.py"

    Returns:
        Number of pixels.
    """

    grid_shape = np.array(distribution_params['grid_shape'])
    grid_vertical, grid_horizontal = generate_grid(original_shape, grid_shape)

    max_grid_size = 0
    for i, j in np.ndindex(*grid_shape):
        width = grid_vertical[i][1] - grid_vertical[i][0]
        height = grid_horizontal[j][1] - grid_horizontal[j][0]
        max_grid_size = max(max_grid_size, width * height)

    return max_grid_size
